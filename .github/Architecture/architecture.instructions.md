Architectural Blueprint for High-Performance House Price Prediction System1. Executive SummaryThis architectural report delineates the strategic design and technical specifications for a robust, scalable House Price Prediction system. In response to the directive to engineer a solution utilizing React with Vite for the frontend, FastAPI for the backend, and Safetensors for a Linear Regression model, this document provides a rigorous analysis of the system’s components, interaction patterns, and deployment infrastructure.The proposed architecture adopts a unified, containerized microservice pattern optimized for serverless deployment on Google Cloud Run. This selection leverages the "scale-to-zero" cost efficiency of serverless computing while maintaining the high-concurrency capabilities required for real-time inference. The persistence layer is architected around PostgreSQL, specifically utilizing the Neon serverless driver to align with the application's stateless scaling characteristics.Key architectural decisions include the use of Safetensors for zero-copy, secure model loading, eliminating the arbitrary code execution risks associated with legacy formats like Pickle. The backend leverages FastAPI’s asynchronous event loop to handle non-blocking database logging via Background Tasks, ensuring that inference latency remains deterministic and minimal. The frontend employs React with Vite for rapid asset bundling, integrated with virtualization libraries to handle high-cardinality real estate datasets efficiently.This report serves as the definitive technical guide for the implementation, covering data schema design, interface contracts, security protocols, and operational monitoring strategies.2. Architectural Paradigm and System Design2.1 The Hexagonal Architecture ApproachTo ensure long-term maintainability and distinct separation of concerns, the system adopts a Hexagonal Architecture (Ports and Adapters). This pattern isolates the core domain logic—price prediction algorithms and business rules—from external technologies such as the database, user interface, and APIs.The application core interacts with the outside world through "Ports," while "Adapters" translate data between the external components and the internal logic. For this project:Primary Adapters (Driving): The FastAPI layer serves as the primary adapter, receiving HTTP requests from the React frontend and converting them into domain objects (Pydantic models).Secondary Adapters (Driven): The persistence layer acts as a secondary adapter, where the abstract repository interface is implemented by SQLAlchemy to communicate with PostgreSQL. Similarly, the Inference Engine adapts the file-system-based Safetensors artifacts into usable mathematical functions.This decoupling allows for independent scaling. The stateless nature of the HTTP adapter (FastAPI) and the Inference adapter (Numpy/Safetensors) enables the system to handle burst traffic by simply adding more container instances, without requiring changes to the core business logic.12.2 System Component DiagramThe high-level data flow operates as follows:Client Layer: A React Single Page Application (SPA) compiled by Vite. It handles user input, visualization, and state management.Edge Layer: Google Cloud Load Balancer (managed by Cloud Run) handles TLS termination and traffic distribution.Application Layer: A monolithic container running FastAPI. It serves two distinct functions:Static Asset Server: Delivers the compiled React application (HTML/JS/CSS) for the initial load.2API Provider: Processes JSON payloads for prediction and feedback.Domain Layer:Inference Service: Loads .safetensors via memory mapping and executes linear algebra operations using Numpy.Logging Service: Offloads request tracing to background threads.Persistence Layer: PostgreSQL (Neon) stores historical predictions, user feedback, and dynamic configuration data (e.g., supported zip codes).2.3 Deployment Strategy SelectionThe request requires selecting a deployment method. After analyzing AWS App Runner, Render, and Google Cloud Run, the architecture selects Google Cloud Run.Comparative Analysis of Deployment PlatformsFeatureGoogle Cloud RunAWS App RunnerRenderScaling ModelRequest-based concurrency (up to 80/instance). Scales to zero.Provisioned instances. Slower to scale.Tier-based scaling. No true scale-to-zero on standard plans.PricingPay-per-use (100ms granularity).Provisioned capacity (higher idle cost).Monthly flat rates (predictable but rigid).IntegrationNative integration with Artifact Registry and Secrets Manager.Strong AWS ecosystem integration.Git-push simplified workflow.SuitabilityOptimal: Matches FastAPI's async concurrency model perfectly.High: Good alternative, but higher latency on cold starts.Medium: Better for varied workloads, less specialized for containerized microservices.Decision: Google Cloud Run is selected for its superior handling of container concurrency, which complements FastAPI’s asynchronous capabilities. Unlike AWS Lambda, which typically handles one request per execution context, Cloud Run allows a single container to process multiple concurrent requests, maximizing the utilization of the underlying CPU for I/O bound tasks like database logging.43. Frontend Engineering Specification: React + ViteThe frontend architecture prioritizes performance metrics such as First Contentful Paint (FCP) and Time to Interactive (TTI), which are critical for user retention in consumer-facing real estate applications.3.1 Build Tooling: Vite vs. WebpackThe choice of Vite over traditional Webpack-based setups (like Create React App) is foundational. Vite leverages native ES Modules (ESM) in the browser during development, bypassing the bundling process entirely for source code. This results in instant server starts and Hot Module Replacement (HMR) regardless of application size.For production, Vite utilizes Rollup, which provides superior tree-shaking capabilities. This is particularly relevant for a real estate application that may import heavy visualization libraries (e.g., charting or mapping). Rollup ensures that unused exports from these libraries are removed from the final bundle, minimizing the JavaScript payload sent to the client.23.2 State Management: React Query & ContextGiven the nature of the application—fetching model predictions based on inputs—the state is divided into Server State and UI State.Server State (TanStack Query): Used for managing asynchronous interactions with the API. When a user queries a price for specific parameters (e.g., "3 Bed, 2 Bath, 90210"), React Query caches this result using the parameters as a composite query key. If the user navigates away and returns, the data is instantly available without a network request. It also handles the isLoading and isError states, simplifying the component logic.7UI State (React Context): Used for global preferences that do not persist to the server, such as theme toggles (Dark/Light mode) or the active step in a multi-stage form wizard.3.3 High-Performance Data Input: List VirtualizationA specific challenge in real estate interfaces is the selection of location data (Neighborhoods, Zip Codes). A standard HTML <select> element rendering 5,000 zip codes creates 5,000 DOM nodes, causing significant layout thrashing and memory usage.Solution: Windowing/VirtualizationThe architecture mandates the use of react-window wrapped within a headless UI component (such as Headless UI's Combobox).Mechanism: Virtualization renders only the items currently visible in the scroll viewport (plus a small buffer). For a list of 10,000 items, only ~10-20 DOM nodes are created at any given time.Performance Impact: This reduces the rendering time from seconds to milliseconds and maintains 60 FPS scrolling performance, crucial for mobile devices.8Implementation Detail:The react-window FixedSizeList component is integrated into the dropdown menu. As the user filters the list via text input, the underlying array passed to react-window updates, and the virtualizer recalculates the visible rows instantly.113.4 Visualization DashboardTo provide actionable insights, the result screen features a dashboard utilizing Recharts.Feature Importance: A horizontal bar chart visualizing the coefficients of the linear regression model (e.g., "Square Footage" contributes +$150k, "Age" contributes -$20k). This "Explainable AI" feature builds trust with the user.Trend Analysis: If historical data is available, a line chart shows price trends for the selected neighborhood over the last 12 months.134. Backend Engineering Specification: FastAPIFastAPI is selected for its high performance, native asynchronous support, and automatic validation via Pydantic. It serves as the orchestration layer, connecting the user input to the inference engine and the database.14.1 Application Lifecycle and Model LoadingTo maximize inference throughput, the machine learning model must be loaded into memory once during the application startup, rather than per request. This is achieved using FastAPI's lifespan event handler.Lifespan Context Manager:Python@asynccontextmanager
async def lifespan(app: FastAPI):
    # Load model artifacts
    model_service.load_model("model.safetensors")
    logger.info("Model loaded successfully into memory.")
    yield
    # Cleanup (if necessary)
    model_service.unload()
This pattern ensures that the overhead of file I/O and memory mapping occurs only when the container starts. The loaded model object is then injected into route handlers as a singleton dependency, ensuring thread safety and efficient resource usage.164.2 Asynchronous Request ProcessingThe architecture distinguishes between CPU-bound tasks (inference) and I/O-bound tasks (logging, database writes).Inference: Linear regression on a single input vector is extremely fast ($O(n)$ where $n$ is features). While technically CPU-bound, the duration is negligible (<1ms). It is executed synchronously within the route handler to return the response immediately.Logging: Writing the prediction details to PostgreSQL is I/O bound and potentially slow (10-100ms). To prevent this from delaying the user response, we utilize FastAPI Background Tasks.Middleware vs. Background Tasks:While Middleware is often used for logging, it typically runs before the response is sent, adding latency. Background Tasks run after the response is returned.Decision: Use Background Tasks for prediction logging. This decouples the API latency from database performance. If the database experiences a momentary slowdown, the user's prediction response is not delayed.174.3 Static File Serving and SPA RoutingSince the frontend and backend are deployed in a unified container, FastAPI must serve the React static assets. A critical requirement for Single Page Applications (SPAs) is handling client-side routing. If a user navigates to /dashboard and refreshes the page, the server receives a request for /dashboard. By default, FastAPI would return a 404 Not Found, as this is not a defined API route.The Catch-All Route Strategy:To support SPA routing, the architecture implements a "catch-all" exception handler or a specific route definition that serves index.html for any unmatched path that does not start with /api.Implementation Pattern:API Routes: Defined under /api/v1/....Static Assets: The /assets folder from the Vite build is mounted to /assets.SPA Fallback: A route {full_path:path} is defined at the root. It checks if a file exists; if not, it returns index.html. This allows React Router to take over in the browser and render the correct view.25. Inference Engine Specification: SafetensorsThe core requirement is to use a Linear Regression model serialized as .safetensors. This format provides significant advantages over legacy formats like Pickle (.pkl) or PyTorch checkpoints (.pt).5.1 Security and Performance AnalysisSecurity (The Anti-Pickle Argument):Pickle is a serialization protocol that allows arbitrary code execution during deserialization. Loading a pickle file from an untrusted source can compromise the entire server. Safetensors is a purely data-centric format; it stores tensors (header + binary data) and cannot execute code. This satisfies the "Best System Architect" requirement for security-first design.20Performance (Zero-Copy Memory Mapping):Safetensors utilizes memory mapping (mmap). When the file is loaded:The JSON header is parsed to determine tensor offsets.The operating system maps the file's binary content directly into the process's virtual address space.Data is not copied to RAM until it is explicitly accessed.For Linear Regression, where the weights matrix is small, this results in near-instantaneous initialization and reduced memory pressure compared to loading a full Python object graph.215.2 Runtime Implementation: Safetensors with NumpyTo maintain a lightweight container, the architecture avoids installing heavy frameworks like PyTorch or TensorFlow. Instead, it relies solely on safetensors and numpy.Inference Logic:The Linear Regression equation is defined as:$$\hat{y} = \mathbf{w}^T \mathbf{x} + b$$Where:$\hat{y}$ is the predicted price.$\mathbf{w}$ is the weight vector loaded from the safetensors file.$\mathbf{x}$ is the feature vector constructed from user input.$b$ is the bias term.The InferenceService class encapsulates this logic:Load: safetensors.numpy.load_file("model.safetensors") returns a dictionary of numpy arrays.Extract: weights = tensors["weight"], bias = tensors["bias"].Compute: np.dot(input_vector, weights) + bias.This approach keeps the Docker image size small (~150MB vs >1GB for PyTorch) and optimizes cold-start times on Cloud Run.205.3 Feature Preprocessing and Dynamic EncodingA critical challenge is ensuring the user input (e.g., "Zipcode: 90210") matches the one-hot encoded vector expected by the model.Problem: If the model was trained with 50 zip codes, the input vector must have length 50 (or 50 + other features).Solution: The .safetensors header allows storing metadata. We utilize this to store the ordered list of feature names. The backend reads this metadata to dynamically map user inputs to vector indices.Dynamic Pydantic Models: To validate inputs against the specific features the model supports, we can generate Pydantic models dynamically at runtime based on the model metadata. This ensures that the API documentation (Swagger UI) always reflects the currently loaded model's capabilities.246. Persistence Layer Architecture: PostgreSQLPostgreSQL is selected as the database engine due to its robustness, ACID compliance, and advanced JSON capabilities.6.1 Database Provider: Neon (Serverless Postgres)For the deployment on Google Cloud Run (a serverless compute platform), a serverless database is the ideal companion. Neon separates storage from compute, allowing the database to scale to zero during periods of inactivity, aligning costs with usage.Architecture Benefits of Neon:Connection Pooling: Neon provides a built-in PgBouncer implementation at the edge. This is crucial for serverless applications where lambda-like functions can spawn thousands of simultaneous connections, potentially overwhelming a standard Postgres instance.26Branching: Neon supports database branching (Copy-on-Write). This allows the CI/CD pipeline to instantly create a clone of the production database for testing migrations before deploying, ensuring high reliability.276.2 Schema Design for ML ObservabilityThe database schema is designed not just for application state, but for Machine Learning Observability (MLOps). We need to track inputs, outputs, and latency to detect model drift.Table: prediction_logsColumnTypeDescriptionidUUIDPrimary Key.timestampTIMESTAMPTZTime of inference. Indexed for time-series analysis.model_versionVARCHAR(50)Identifies which model artifact was used (e.g., "v1.2.0").inputsJSONBStores the raw feature vector (sqft, location, etc.).predictionDECIMAL(12, 2)The output price.latency_msINTEGERExecution time for performance monitoring.The Power of JSONB:Using JSONB for inputs is a strategic architectural decision. It allows the schema to remain flexible. If the next version of the model introduces a new feature (e.g., "Proximity to Subway"), we can log this new feature without altering the database schema (ALTER TABLE). PostgreSQL allows indexing specific keys within the JSONB column, enabling queries like "Find all predictions where sqft > 2000" efficiently.297. Infrastructure and Deployment Strategy7.1 Unified Container StrategyTo simplify the architecture for a "project" scope while maintaining scalability, the system employs a Monolithic Container strategy. Both the compiled React frontend and the FastAPI backend are packaged into a single Docker image.Justification:Simplified Networking: No CORS (Cross-Origin Resource Sharing) configuration is required for production, as the API and Frontend are served from the same origin.Atomic Deployments: The frontend and backend versions are strictly coupled. You never have a scenario where Frontend v2 is trying to talk to Backend v1, eliminating a broad class of deployment errors.Cost Efficiency: Only one Cloud Run service needs to be managed and paid for.7.2 Dockerfile SpecificationThe build utilizes a Multi-Stage Dockerfile to minimize the final image size.32Dockerfile# Stage 1: Build Frontend
FROM node:18-alpine as frontend-builder
WORKDIR /app/frontend
COPY frontend/package*.json./
RUN npm ci
COPY frontend/.
RUN npm run build

# Stage 2: Build Backend & Final Image
FROM python:3.10-slim
WORKDIR /app
# Install dependencies
COPY backend/requirements.txt.
RUN pip install --no-cache-dir -r requirements.txt
# Copy Backend Code
COPY backend/.
# Copy Model Artifact
COPY model.safetensors.
# Copy Built Frontend Assets from Stage 1
COPY --from=frontend-builder /app/frontend/dist /app/static

# Command to run the application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]
7.3 Continuous Integration/Continuous Deployment (CI/CD)The deployment pipeline is triggered via GitHub Actions:Test: Runs Pytest (backend) and Vitest (frontend).Build: Builds the Docker container.Push: Pushes the image to Google Artifact Registry.Deploy: Updates the Google Cloud Run service with the new image revision.Verification: A smoke test hits the /health endpoint to ensure the service is live.8. Security and Compliance8.1 Input Sanitization and ValidationSecurity begins at the edge. Pydantic models act as a strict firewall for data.Type Safety: Ensures sqft is a positive integer.Range Validation: Prevents outliers that could cause numerical instability in the model (e.g., a house with -5 bedrooms).Injection Prevention: By using ORM (SQLAlchemy) and Pydantic, the system is naturally resistant to SQL Injection and Cross-Site Scripting (XSS) attacks on the inputs.8.2 Safe Model HandlingAs previously analyzed, the strict requirement for Safetensors mitigates the risk of Model Serialization Attacks. Unlike Pickle, which effectively allows "Remote Code Execution as a Service" if compromised, Safetensors ensures that only data is loaded. The loader is configured to reject any file that does not conform to the strict header specification.219. Operational Excellence and MLOps9.1 Drift DetectionThe prediction_logs table in PostgreSQL serves as the foundation for monitoring Data Drift. A scheduled job (e.g., using Cloud Scheduler) can run SQL queries to calculate the distribution of input features (mean sqft, distribution of zip codes) over the last week and compare them to the training baseline. Significant deviation triggers an alert, indicating the model may need retraining.339.2 Feedback LoopTo close the loop, the frontend includes a "Feedback" mechanism. After a price is predicted, the user can click "Too High" or "Too Low". This data is stored in the feedback table linked to the prediction_id. This "Ground Truth" proxy is invaluable for evaluating model performance in the wild without waiting for actual sales data.3510. ConclusionThis architectural blueprint satisfies the request for a high-performance House Price Prediction system by integrating best-in-class technologies. React and Vite provide a responsive, optimized user interface capable of handling complex location data through virtualization. FastAPI drives the backend with high-concurrency asynchronous processing, ensuring that database logging via Neon does not impact user-perceived latency. The use of Safetensors and Numpy creates a secure, efficient inference engine that avoids the overhead of deep learning frameworks. Finally, the Google Cloud Run deployment strategy wraps this robust stack in a cost-effective, serverless container, ready to scale from zero to thousands of users instantly.Detailed Technical Specification11. Frontend Implementation Details11.1 Vite Configuration for ProxyingDuring development, the frontend (port 5173) and backend (port 8000) run on different ports. To avoid CORS issues and mimic production behavior, Vite's proxy capability is utilized.vite.config.ts Specification:TypeScriptimport { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

export default defineConfig({
  plugins: [react()],
  server: {
    proxy: {
      '/api': {
        target: 'http://localhost:8000',
        changeOrigin: true,
        secure: false,
      }
    }
  },
  build: {
    outDir: '../backend/static', // Build directly to backend static folder
    emptyOutDir: true,
  }
})
This configuration directs all requests starting with /api to the FastAPI backend, creating a seamless development environment.211.2 Component Architecture: Virtualized Location SelectorThe implementation of the location selector is a critical performance optimization. We utilize react-window to create a virtualized list.Component Logic:Data Fetch: useQuery fetches the list of supported zip codes from /api/v1/locations.Filtering: A local state query filters this list based on user input.Rendering: The filtered list is passed to FixedSizeList.Interaction: Selecting an item updates the react-hook-form state.Code Structure:TypeScriptimport { FixedSizeList as List } from 'react-window';
import { useQuery } from '@tanstack/react-query';

const LocationSelect = ({ control }) => {
  const { data: locations } = useQuery({ queryKey: ['locations'], queryFn: fetchLocations });
  
  // Row component for virtualization
  const Row = ({ index, style }) => (
    <div style={style} onClick={() => selectLocation(locations[index])}>
      {locations[index].name}
    </div>
  );

  return (
    <List
      height={200}
      itemCount={locations?.length |

| 0}
      itemSize={35}
      width={'100%'}
    >
      {Row}
    </List>
  );
};
This ensures the UI remains responsive even if the model supports 20,000 distinct locations.1012. Backend Implementation Details12.1 FastAPI Main Entry Point & SPA HandlingThe main.py serves as the convergence point. It configures middleware, includes routers, and sets up the static file serving.SPA Catch-All Implementation:To support React Router's pushState navigation (e.g., users navigating directly to https://app.com/results), the backend must serve index.html for non-API routes.Pythonfrom fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
import os

app = FastAPI()

# Mount API routes first
app.include_router(api_router, prefix="/api/v1")

# Mount static assets (JS/CSS)
app.mount("/assets", StaticFiles(directory="static/assets"), name="assets")

# Catch-all route for SPA
@app.get("/{full_path:path}")
async def serve_spa(full_path: str):
    # Check if file exists in static (e.g., favicon.ico)
    file_path = os.path.join("static", full_path)
    if os.path.exists(file_path) and os.path.isfile(file_path):
        return FileResponse(file_path)
    # Otherwise, return index.html
    return FileResponse("static/index.html")
This implementation ensures that API requests are handled correctly, static assets are served efficiently, and client-side routes fallback to the React application entry point.212.2 Dynamic Pydantic Models for One-Hot EncodingThe Linear Regression model relies on One-Hot Encoding for categorical features. The backend must enforce that only valid categories (present in the training data) are accepted.Dynamic Enum Generation:Instead of hardcoding the Location enum, which creates maintenance debt, we load it from the model metadata.Pythonfrom enum import Enum
from pydantic import BaseModel, create_model

# Load metadata from safetensors header or config.json
valid_locations = # Loaded at startup

# Create dynamic Enum
LocationEnum = Enum('LocationEnum', {loc: loc for loc in valid_locations})

# Dynamic Request Model
class PredictionRequest(BaseModel):
    sqft: float
    location: LocationEnum # Pydantic will now validate against the dynamic list
This ensures that if the model is updated with new locations, the API validation logic updates automatically upon restart without code changes.2413. Database Schema and Performance Tuning13.1 SQL DefinitionThe PostgreSQL schema is defined using SQLAlchemy Core or ORM.SQLCREATE TABLE prediction_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    model_version VARCHAR(50) NOT NULL,
    input_payload JSONB NOT NULL,
    predicted_price NUMERIC(15, 2) NOT NULL,
    execution_time_ms INTEGER
);

-- Index for temporal queries (Dashboard analytics)
CREATE INDEX idx_logs_created_at ON prediction_logs (created_at);

-- Index for querying specific features within JSONB
CREATE INDEX idx_logs_sqft ON prediction_logs ((input_payload->>'sqft'));
13.2 Connection Pooling ConfigurationWhen running on Cloud Run, the application instances scale up and down rapidly. Direct connections can exhaust the database limits. Using AsyncPG with SQLAlchemy requires specific tuning.Configuration:Pool Size: Set to 5-10 per container. Cloud Run handles concurrency within the container, so we don't need a connection per request.Max Overflow: Set to 10 to handle bursts.Pool Timeout: 30 seconds.For Neon, since it exposes a pooling endpoint (port 6432 typically), the application should connect to this endpoint. The connection string in config.py should point to the pooled URL. This allows thousands of Cloud Run instances to share the underlying physical connections to the Postgres compute node.2614. Deployment Pipeline (CI/CD)The reliability of the system is guaranteed by an automated pipeline.GitHub Actions Workflow:Linting & Type Checking:ruff check. (Python linting)mypy. (Python static type checking)npm run lint (Frontend linting)Unit Testing:pytest runs backend logic tests (mocking the database and model loading).vitest runs frontend component tests.Integration Testing:A test database is spun up (using a Service Container in GitHub Actions).The API is started.A real request is made, checking that it flows to the database and back.Delivery:docker build creates the unified artifact.gcloud run deploy pushes the new version.Traffic Splitting: For safety, a canary deployment strategy can be used. gcloud run services update-traffic --to-tags new-revision=10 sends 10% of traffic to the new version to monitor for errors before a full rollout.15. ConclusionThis comprehensive architectural report outlines a state-of-the-art system for House Price Prediction. By adhering to the constraints—React/Vite, FastAPI, Safetensors—and making strategic decisions for the remaining components (PostgreSQL/Neon, Cloud Run), the design achieves a balance of performance, scalability, and maintainability.The system leverages virtualization on the frontend to solve data cardinality issues, asynchronous background tasks on the backend to solve latency issues, and zero-copy serialization in the inference engine to solve memory and security issues. The result is a professional-grade application architecture capable of serving high-demand workloads with minimal operational overhead.